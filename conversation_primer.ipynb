{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "734721a0"
   },
   "source": [
    "# MSAI 495 | Text Generation | Conversation Primer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "e5987706"
   },
   "source": [
    "### Business Goal / Case Statement\n",
    "\n",
    "My goal is to elevate user engagement and retention in social and messaging applications by fine-tuning a transformer-based language model (GPT-2) on a rich dataset of 17,000+ curated conversation prompts. This model will enable the dynamic generation of relevant, high-quality conversation starters—ranging from light and playful to deep and thought-provoking—tailored to the context and interests of users.\n",
    "\n",
    "This AI-driven conversation engine directly addresses three key challenges faced by social apps:\n",
    "\n",
    "* **Breaking the Ice Faster**: Reduces the awkwardness of initiating conversations, especially in dating or friend-finding platforms, improving onboarding and first-session engagement.\n",
    "\n",
    "* **Personalizing Conversations at Scale**: By leveraging tagged prompts and topical filters, the model serves up prompts aligned with user interests and emotional tone, increasing time spent in-app.\n",
    "\n",
    "* **Enhancing Social Stickiness and Retention**: Consistent access to fresh, engaging conversation content encourages daily interactions and helps develop meaningful user connections—driving higher session frequency and lower churn.\n",
    "\n",
    "Initially, this model will serve as an embedded feature within new group chats and newly formed direct messages. Long-term, it will function as the foundational layer for an AI-powered conversation assistant integrated across mobile and web apps. This assistant will deliver intelligent prompts in real-time, encouraging authentic connections.\n",
    "\n",
    "### Assignment Context\n",
    "\n",
    "**Relevant Industry and/or Business Function:** Social and messaging apps\n",
    "\n",
    "**Description:**\n",
    "\n",
    "To enhance conversational engagement across social and messaging platforms, the core task involves fine-tuning the GPT-2 model to generate contextual and tagged conversation starters, enabling:\n",
    "\n",
    "* Prompt personalization via topic filtering\n",
    "\n",
    "* Real-time generation of icebreakers, discussion questions, or thought starters\n",
    "\n",
    "* Low-latency deployment for integration in messaging interfaces\n",
    "\n",
    "By enabling AI-driven, context-aware conversations, this project empowers product teams to deliver more meaningful, dynamic, and human-like interactions.\n",
    "\n",
    "### The Data\n",
    "\n",
    "**Dataset name:** <code>[conversation-starters](https://huggingface.co/datasets/Langame/conversation-starters)</code><br>\n",
    "\n",
    "**Data characteristics**\n",
    "\n",
    "* 17,470 diverse prompts with topic tags\n",
    "\n",
    "* Wide range of conversation depths (casual to profound)\n",
    "\n",
    "* Multiple topic categories for targeted generation\n",
    "\n",
    "* Varying prompt lengths and complexity\n",
    "\n",
    "### Model Architecture(s)\n",
    "\n",
    "* GPT-2 Medium/Large: For autoregressive generation of conversation starters\n",
    "\n",
    "### AI/ML Task(s)\n",
    "\n",
    "Fine-tune a pre-trained transformer model (GTP-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "a6dd0a1e"
   },
   "source": [
    "## Step 1: Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3",
   "metadata": {
    "executionInfo": {
     "elapsed": 34908,
     "status": "ok",
     "timestamp": 1748296439241,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "x3ZhkNOp9L8P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "b3e08711"
   },
   "source": [
    "## Step 2: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2065,
     "status": "ok",
     "timestamp": 1748296441304,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "d8c52d1a",
    "outputId": "800eeaf7-e050-43de-f7b8-91e38d45f745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['topics', 'prompt'],\n",
      "        num_rows: 17470\n",
      "    })\n",
      "})\n",
      "\n",
      "First few examples:\n",
      "{'topics': [['video games'], ['science'], ['relationship'], ['personal', 'relationship', 'relationships', 'social', 'big talk', 'personal growth'], ['transhumanism', 'fun']], 'prompt': ['What was the most difficult aspect of mastering a video game?', 'What scientific or intellectual studies do you think would increase substantially if more people contributed around them?', 'What would you do if your partner disappeared?', 'Give eachother four praises and one critique', 'If a sufficiently advanced robot were to take your place in society and have children with a sufficiently advanced robot, would the children have any advantages over you?']}\n",
      "\n",
      "Column names: ['topics', 'prompt']\n",
      "\n",
      "Dataset size: 17470\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the conversation starters dataset\n",
    "dataset = load_dataset(\"Langame/conversation-starters\")\n",
    "\n",
    "# Explore the dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)\n",
    "print(\"\\nFirst few examples:\")\n",
    "print(dataset['train'][:5])\n",
    "\n",
    "# Check the columns and data format\n",
    "print(\"\\nColumn names:\", dataset['train'].column_names)\n",
    "print(\"\\nDataset size:\", len(dataset['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "4W3FnL4BMtob"
   },
   "source": [
    "## Step 3: Data Preprocessing and Tokenization\n",
    "\n",
    "Preprocess the conversation starters into a format suitable for GPT-2 training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1748296441406,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "7V_6vVnoM6yk",
    "outputId": "f2c1955d-c312-431a-fb47-3638942b3e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-formatting dataset with improved structure...\n",
      "Sample formatted text:\n",
      "<|startoftext|>Topic: video games\n",
      "Conversation Starter: What was the most difficult aspect of mastering a video game?<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "def format_conversation_data(examples):\n",
    "    \"\"\"\n",
    "    Format the data for GPT-2 training\n",
    "    \"\"\"\n",
    "    formatted_texts = []\n",
    "\n",
    "    for topics, prompt in zip(examples['topics'], examples['prompt']):\n",
    "        # Handle topics properly\n",
    "        if topics and len(topics) > 0:\n",
    "            topic_str = \", \".join(topics[0]) if isinstance(topics[0], list) else str(topics[0])\n",
    "        else:\n",
    "            topic_str = \"general\"\n",
    "\n",
    "        # Use a clearer format with special tokens\n",
    "        formatted_text = f\"<|startoftext|>Topic: {topic_str}\\nConversation Starter: {prompt}<|endoftext|>\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "    return {\"text\": formatted_texts}\n",
    "\n",
    "# Re-format your dataset\n",
    "print(\"Re-formatting dataset with improved structure...\")\n",
    "formatted_dataset = dataset['train'].map(format_conversation_data, batched=True)\n",
    "formatted_dataset = formatted_dataset.remove_columns(['topics', 'prompt'])\n",
    "\n",
    "print(\"Sample formatted text:\")\n",
    "print(formatted_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "i6Rou6Jn9jNH"
   },
   "source": [
    "## Step 4: Model and Tokenizer Setup\n",
    "\n",
    "Load the GPT-2 model and tokenizer, and configure them for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5990,
     "status": "ok",
     "timestamp": 1748296447392,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "FSwqZuri9qAq",
    "outputId": "63243d6d-cf71-4171-d235-b2dbb40d57f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: gpt2\n",
      "Added 3 special tokens\n",
      "New vocabulary size: 50260\n",
      "Model parameters: 124,442,112\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also use \"gpt2-medium\" for better performance\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Add special tokens BEFORE resizing embeddings\n",
    "special_tokens = {\n",
    "    \"additional_special_tokens\": [\"<|startoftext|>\", \"<|topic|>\", \"<|starter|>\"]\n",
    "}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Added {num_added_tokens} special tokens\")\n",
    "print(f\"New vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "Qe_hfMXp9yL1"
   },
   "source": [
    "## Step 5: Tokenization Function\n",
    "\n",
    "Create a function to tokenize formatted text data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "d935fe2e0e09477f9b63d45d568be6b5",
      "b690b01d707a4e7799ae42bf9cc99a8b",
      "7775e1aaaf5345eba9045aa3b6da5e62",
      "e9e3a9fa95ac4907b472e77d1864604b",
      "3128226eff954bfc8dc919b1447da867",
      "dec06d77e8124cffa4922e8351515aad",
      "f915731a80f847c9b885eb31b49fcc43",
      "3530e555e1104402be470477b73fe3bd",
      "d98cc9ff3a794bf7bf845b76784ae742",
      "f4689af598714ac5863c3c75b1872600",
      "13d819a15cb941adae0631376d1bc068"
     ]
    },
    "executionInfo": {
     "elapsed": 23550,
     "status": "ok",
     "timestamp": 1748296470944,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "ZjHYx3UH9608",
    "outputId": "32c9f048-1661-4cef-c1f1-17b4e3c936fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-tokenizing dataset with proper padding...\n",
      "Re-tokenization complete!\n",
      "Sample lengths should now be consistent: 512\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the text data for GPT-2 training\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",  # Pad all sequences to max_length\n",
    "        truncation=True,\n",
    "        max_length=512,  # Adjust based on your needs and GPU memory\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Set labels as lists (not tensor clones)\n",
    "    tokenized[\"labels\"] = [list(ids) for ids in tokenized[\"input_ids\"]]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Re-apply the corrected tokenization\n",
    "print(\"Re-tokenizing dataset with proper padding...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Recreate the train/test split\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"Re-tokenization complete!\")\n",
    "print(f\"Sample lengths should now be consistent: {len(tokenized_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "pB14cdEU-GWs"
   },
   "source": [
    "## Step 6: Data Splitting and Data Collator Setup\n",
    "\n",
    "Split dataset into training and validation sets, and set up the data collator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1748296470956,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "gG4rn7L6-PRU",
    "outputId": "17211776-d7aa-4c3d-f18e-a7afb372a7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 15723\n",
      "Validation samples: 1747\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and validation sets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Set up data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # GPT-2 uses causal language modeling, not masked language modeling\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "A7jbB-D5-aWf"
   },
   "source": [
    "Step 7: Training Arguments Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1748296471006,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "h3flFkFY-eUk",
    "outputId": "ecbede77-19a2-4280-a722-fe75cc3e03a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured!\n",
      "Effective batch size: 8\n",
      "Total training steps: 1965\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-conversation-starters\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # Reduced from 3\n",
    "    per_device_train_batch_size=2,  # Reduced from 4\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Increased to maintain effective batch size\n",
    "    warmup_steps=100,  # Reduced from 500\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=50,   # More frequent logging\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,     # More frequent evaluation\n",
    "    save_steps=500,     # More frequent saving\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    fp16=True,\n",
    "    report_to=None,\n",
    "    max_steps=1000,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "OH_ZXXj_-zWN"
   },
   "source": [
    "## Step 8: Initialize the Trainer\n",
    "\n",
    "Set up the Trainer object that will handle the fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1748296471113,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "I8mpPMO9-4tu",
    "outputId": "be0cb998-e6c4-4b4e-a73a-c2a1a1600885"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4405/3111867214.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "Training dataset size: 15723\n",
      "Evaluation dataset size: 1747\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training dataset size: {len(trainer.train_dataset)}\")\n",
    "print(f\"Evaluation dataset size: {len(trainer.eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "RZBvzLAa_CCU"
   },
   "source": [
    "## Step 9: Start Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100-PCIE-40GB\n",
      "Memory: 42.4 GB\n"
     ]
    }
   ],
   "source": [
    "# Check GPU status\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Begin the actual training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 324992,
     "status": "error",
     "timestamp": 1748296796106,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "LzxieMcH_Gnm",
    "outputId": "d9e32eda-56de-42cb-8ead-c17fb94d7bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "This may take 30-60 minutes depending on your GPU...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.075100</td>\n",
       "      <td>1.851705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.915700</td>\n",
       "      <td>1.785096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.769300</td>\n",
       "      <td>1.757037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.745900</td>\n",
       "      <td>1.725877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.851500</td>\n",
       "      <td>1.716175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Final training loss: 1.9416\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This may take 30-60 minutes depending on your GPU...\")\n",
    "\n",
    "# Train the model\n",
    "training_result = trainer.train()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final training loss: {training_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "zT4oOYD3OPQY"
   },
   "source": [
    "## Step 10: Save and Test Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1748296796111,
     "user": {
      "displayName": "Matthew Yao",
      "userId": "15262283235916685442"
     },
     "user_tz": 300
    },
    "id": "PmgvQx0JOULC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the fine-tuned model...\n",
      "Model saved successfully!\n",
      "\n",
      "=== Testing Generation of Conversation Primers ===\n",
      "\n",
      "Topic: relationships\n",
      "Generated: What do you think are the most important aspects of a relationship?\n",
      "\n",
      "Topic: science\n",
      "Generated: What is the most beautiful thing you have ever seen?\n",
      "\n",
      "Topic: video games\n",
      "Generated: What is the most challenging game you have played?\n",
      "\n",
      "Topic: philosophy\n",
      "Generated: How do you think the world will change if we stop using religion to teach us how to live?\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving the fine-tuned model...\")\n",
    "trainer.save_model(\"./gpt2-conversation-starters-final\")\n",
    "tokenizer.save_pretrained(\"./gpt2-conversation-starters-final\")\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Test your fine-tuned model\n",
    "def generate_conversation_starter(topic, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate a conversation starter for a given topic\n",
    "    \"\"\"\n",
    "    # Use the exact training format\n",
    "    prompt = f\"<|startoftext|>Topic: {topic}\\nConversation Starter:\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    attention_mask = torch.ones_like(inputs)  # Explicit attention mask\n",
    "\n",
    "    # Generate with stricter parameters\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            attention_mask=attention_mask,  # Add attention mask\n",
    "            max_length=inputs.shape[1] + 30,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # Remove early_stopping flag\n",
    "        )\n",
    "\n",
    "    # Decode and extract just the conversation starter\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the conversation starter part\n",
    "    if \"Conversation Starter:\" in generated_text:\n",
    "        starter = generated_text.split(\"Conversation Starter:\")[-1].strip()\n",
    "        # Clean up - stop at first sentence\n",
    "        if \"?\" in starter:\n",
    "            starter = starter.split(\"?\")[0] + \"?\"\n",
    "        elif \".\" in starter:\n",
    "            starter = starter.split(\".\")[0] + \".\"\n",
    "        return starter\n",
    "\n",
    "    return \"Could not generate conversation starter\"\n",
    "\n",
    "# Test the improved version\n",
    "test_topics = [\"relationships\", \"science\", \"video games\", \"philosophy\"]\n",
    "\n",
    "print(\"\\n=== Testing Generation of Conversation Primers ===\")\n",
    "for topic in test_topics:\n",
    "    starter = generate_conversation_starter(topic)\n",
    "    print(f\"\\nTopic: {topic}\")\n",
    "    print(f\"Generated: {starter}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
